# Design Document for Gigapaxos based Replicated Datastore (PA 4.1)

This project implements a replicated datastore using Gigapaxos Replicated State Machine interface and the only file modified for this part of the assignment is MyDBReplicableAppGP.java which provides application logic that Gigapaxos replicates across the servers. Each replica connects to its own Cassandra keyspace and is created on startup if it does not already exist in there and all the CQL commands delivered through Gigapaxos are executed deterministically against that keyspace because Gigapaxos totally orders all client requests and all replicas apply the same sequence of operations and remain consistent.

The execute method extracts a CQL command from the incoming RequestPacket and submits it to Cassandra and the non CQL requests are safely ignored. A simple request counter tracks how many committed operations have been applied and this metadata is used for checkpointing. The checkpoint method returns a compact string that encodes the keyspace and the request count while the restore method parses this string on startup to reset the counter and cassandra gets the actual table contents so no extra work is needed to restore stored data.

Basically this design meets the requirements of a deterministic replicated application under Gigapaxos and the application executes all operations in order and maintains identical state across replicas and provides meaningful checkpoint and restore functionality. Consistency tests run successfully with this implementation and the replication behavior matches the expectations of the Gigapaxos RSM model.

# Design Document for Zookeeper based Fault Tolerant Server (PA 4.2)

This implementation uses Zookeeper to provide a total order broadcast mechanism that keeps all replicas consistent and instead of performing leader election or implementing a traditional consensus algorithm the system relies on Zookeepers persistent sequential znodes to serialize all the client operations. Whenever a client sends a command to any server that server creates a new sequential znode under proposals containing the request ID and CQL command because Zookeeper assigns an increasing sequence number to each proposal so this mechanism creates a global ordering of operations regardless of which server receives the client request.

Each server installs a watcher on /proposals and wen Zookeeper notifies a server that a new proposal has been added the server retrieves all proposal znodes and sorts them and executes any proposals whose sequence number is higher than the servers last executd one. The commands are applied directly to the local Cassandra keyspace that stores the servers state and if a proposal originated locally then the server sends an acknowledgment back to the appropriate client. All replicas execute proposals in exactly the same order which is ensuring consistency without requiring inter server messaging.

Crash recovery is supported using a checkpoint stored in a persistent znode /checkpoint. Every 100 executed proposals a server updates this checkpoint with the name of the last executed proposal and an execution count. After a crash a server reads this checkpoint during initialization and replays only the proposals created after the checkpoint and this allows a recovering server to catch up to the global state by re executing missed operations.

The design maintains only minimal in memory state which storing only pending client requests while Zookeeper provides the durable ordering log and Cassandra stores the durable application state. Although the checkpoint is global and not per server and the proposal log is never garbage collected the implementation still achieves deterministic execution ordering and consistency across replicas and functional crash recovery for the assignmentâ€™s testing environment.

# Design Document for Apache Ratis based Fault Tolerant Server (PA 4.3 Extra Credit)

This implementation uses Apache Ratis which is a Java library that implements the Raft consensus protocol to build a self contained fault tolerant replicated database without requiring external coordination services. The servers form a Raft cluster where one server acts as leader and handles client requests while the followers replicate the leaders log and the Raft Server component handles leader election and log replication and crash recovery automatically while the DatabaseStateMachine executes committed CQL commands on Cassandra.

So when a client sends a CQL command to any server that server basically creates a RaftClientRequest that contains a command that is wrapped in JSON with a unique request ID and submits it to the Raft cluster. The leader appends this request to its log and replicates it to its followers using gRPC and once a majority of servers have the entry the leader commits it and Ratis invokes applyTransactionSerial on each servers state machine which extracts the CQL command and executes it against Cassandra. The server that received the request first sends the response back to the client while other servers simply just execute without responding.
Crash recovery works through persistent Raft logs stored in raft_logs/serverX directories which survive crashes and when a server restarts it initializes with the same fixed RaftGroupId so other servers recognize it and the server reads its log and contacts other servers to catch up on missed entries and if the leader crashes the remaining servers elect a new leader within 300 to 500 milliseconds and continue operating. Cassandra data stays safe across crashes so recovered servers only need to replay uncommitted log entries.

Checkpointing is simplified because the takeSnapshot method returns the executedCount which tells Ratis how far the execution has progressed and Ratis uses this information to compact old log entries and keep the memory bounded. So consistency is maintained because Raft ensures all servers execute commands in the same order and availability requires a majority of servers running and the implementation required resolving 17 JAR files for Ratis and its dependencies totaling 42 megabytes. The main challenges involved getting correct API calls for Ratis 2.5.1 and managing client response routing which was solved with a ConcurrentHashMap mapping request IDs to client addresses.